# Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ§‹é€ ãƒ»ãƒ­ã‚¸ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆ

**ä½œæˆæ—¥:** 2026-01-08  
**å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª:** `Assets/StreamingAssets/`

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ§‹é€ ã€å½¹å‰²ã€ãŠã‚ˆã³ä¸»è¦ãªãƒ­ã‚¸ãƒƒã‚¯ãƒ•ãƒ­ãƒ¼ã‚’è©³ç´°ã«è§£èª¬ã—ã¾ã™ã€‚

---

## 1. ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```mermaid
graph TD
    subgraph Unity["Unity (C#)"]
        PL[PythonLauncher.cs] -->|stdin| MainPy
    end

    subgraph Python["Python ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰"]
        MainPy[main_vision_voice.py] --> Camera[camera_capture.py]
        MainPy --> YOLO[yolo_processor.py]
        MainPy --> Ollama[ollama_client.py]
        MainPy --> DeepSeek[deepseek_client.py]
        MainPy --> Voice[voice_client.py]
        MainPy --> Prompts[prompts.py]
    end

    subgraph External["å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹"]
        Ollama -->|ãƒ­ãƒ¼ã‚«ãƒ«| OllamaAPI((Ollama qwen2.5vl:7b))
        DeepSeek -->|HTTPS| DeepSeekAPI((DeepSeek API))
        Voice -->|ãƒ­ãƒ¼ã‚«ãƒ«| COEIROINK((COEIROINK))
    end
```

---

## 2. å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```mermaid
sequenceDiagram
    participant Unity
    participant Main as main_vision_voice.py
    participant Cam as camera_capture.py
    participant YOLO as yolo_processor.py
    participant Ollama as ollama_client.py
    participant DS as deepseek_client.py
    participant Voice as voice_client.py

    Unity->>Main: CAPTURE <index>
    Main->>Main: print("[[STATE_START]]")
    
    Main->>Cam: capture_with_stabilization()
    Cam-->>Main: frame (numpy array)
    
    Main->>YOLO: detect_and_crop(frame)
    YOLO-->>Main: cropped_image, detection_info
    
    Main->>Main: apply_intelligent_brightness()
    Main->>Main: CLAHEå‡¦ç†
    Main->>Main: rembgèƒŒæ™¯é™¤å»
    
    Main->>Ollama: analyze_image(path)
    Ollama-->>Main: {is_machine, shape, state, item_name}
    
    Main->>Main: print("[[STATE_COMPLETE]]")
    
    Main->>Main: determine_persona(analysis_data)
    Main->>DS: generate_dialogue(item_name, context, topic)
    DS-->>Main: "ã‚»ãƒªãƒ• by ã‚­ãƒ£ãƒ©å"
    
    Main->>Main: print("[[CHARACTER]] ã‚­ãƒ£ãƒ©å")
    Main->>Main: print("[[CREDIT]] by OO")
    Main->>Main: print("[[MESSAGE]] ã‚»ãƒªãƒ•")
    
    Main->>Voice: synthesize(text, uuid)
    Voice-->>Main: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
```

---

## 3. ã‚¹ã‚¯ãƒªãƒ—ãƒˆè©³ç´°

### ğŸ”µ ãƒ¡ã‚¤ãƒ³ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼

#### [main_vision_voice.py](file:///Users/asanolab/Sotsusei1107/Assets/StreamingAssets/main_vision_voice.py)
**å½¹å‰²:** å…¨ä½“ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’çµ±æ‹¬ã€‚stdinç›£è¦–ã€å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã€stdoutå‡ºåŠ›

**ä¸»è¦é–¢æ•°:**

| é–¢æ•°å | å½¹å‰² |
|:---|:---|
| `stdin_listener()` | Unityã‹ã‚‰ã®stdinã‚³ãƒãƒ³ãƒ‰ã‚’ç›£è¦–ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ |
| `process_frame(frame)` | ã‚«ãƒ¡ãƒ©ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ï¼ˆYOLOâ†’å‰å‡¦ç†â†’åˆ†æï¼‰ |
| `_process_analysis(analysis_data, filename)` | åˆ†æçµæœã‹ã‚‰ã‚»ãƒªãƒ•ç”Ÿæˆãƒ»éŸ³å£°åˆæˆ |
| `determine_persona(analysis_data)` | 5æ®µéšå„ªå…ˆåº¦ãƒ­ã‚¸ãƒƒã‚¯ã§ãƒšãƒ«ã‚½ãƒŠæ±ºå®š |
| `get_voice_uuid(persona_id)` | ãƒšãƒ«ã‚½ãƒŠIDã‹ã‚‰éŸ³å£°UUIDã‚’ãƒãƒƒãƒ”ãƒ³ã‚° |
| `_save_message_pair(image, message, credit)` | MessagePairs.jsonã«ä¿å­˜ |

**stdinã‚³ãƒãƒ³ãƒ‰å‡¦ç†:**
```python
def stdin_listener():
    while True:
        line = sys.stdin.readline().strip()
        if line.startswith("CAPTURE"):
            parts = line.split()
            camera_index = int(parts[1]) if len(parts) > 1 else 0
            
            print("[[STATE_START]]")
            frame = camera_capture.capture_with_stabilization()
            # ... å‡¦ç†ç¶šè¡Œ
        elif line == "QUIT":
            break
```

**ãƒšãƒ«ã‚½ãƒŠæ±ºå®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆ5æ®µéšå„ªå…ˆåº¦ï¼‰:**
```python
def determine_persona(analysis_data):
    # 1. ã‚¢ã‚¤ãƒ†ãƒ å›ºæœ‰ã®ãƒšãƒ«ã‚½ãƒŠ (item_obsessions.py)
    # 2. æ©Ÿæ¢°/é›»å­æ©Ÿå™¨ â†’ "Observer"
    # 3. å½¢çŠ¶ãƒ™ãƒ¼ã‚¹ (Round/Sharp/Square)
    # 4. çŠ¶æ…‹ãƒ™ãƒ¼ã‚¹ (Old/Dirty/Broken)
    # 5. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ â†’ "Neutral"
    return (persona_id, role_name_jp)
```

---

### ğŸŸ¢ ã‚«ãƒ¡ãƒ©åˆ¶å¾¡

#### [camera_capture.py](file:///Users/asanolab/Sotsusei1107/Assets/StreamingAssets/camera_capture.py)
**å½¹å‰²:** ãƒ•ãƒªãƒƒã‚«ãƒ¼å¯¾ç­–ä»˜ãã‚«ãƒ¡ãƒ©æ’®å½±ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

**CameraCaptureã‚¯ãƒ©ã‚¹:**

| ãƒ¡ã‚½ãƒƒãƒ‰ | å‹•ä½œ |
|:---|:---|
| `__init__()` | ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰ã§ã‚«ãƒ¡ãƒ©åˆæœŸåŒ–ï¼ˆéœ²å‡ºãƒ»ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆè¨­å®šï¼‰ |
| `initialize()` | ã‚«ãƒ¡ãƒ©ãƒ‡ãƒã‚¤ã‚¹ã‚’é–‹ãã€è¨­å®šã‚’é©ç”¨ |
| `capture_with_stabilization()` | 5ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ— + 5ãƒ•ãƒ¬ãƒ¼ãƒ ä¸­å¤®å€¤åˆæˆ |
| `capture_single()` | å˜ç´”ãª1ãƒ•ãƒ¬ãƒ¼ãƒ ã‚­ãƒ£ãƒ—ãƒãƒ£ï¼ˆãƒ•ãƒªãƒƒã‚«ãƒ¼å¯¾ç­–ãªã—ï¼‰ |

**ãƒ•ãƒªãƒƒã‚«ãƒ¼å¯¾ç­–ãƒ­ã‚¸ãƒƒã‚¯:**
```python
def capture_with_stabilization(self, warmup_frames=5, capture_frames=5):
    # 1. éœ²å‡ºå®‰å®šã®ãŸã‚ warmup_frames ãƒ•ãƒ¬ãƒ¼ãƒ æ¨ã¦ã‚‹
    for _ in range(warmup_frames):
        self.cap.read()
        time.sleep(0.05)
    
    # 2. capture_frames ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—
    frames = []
    for _ in range(capture_frames):
        ret, frame = self.cap.read()
        if ret:
            frames.append(frame)
    
    # 3. ä¸­å¤®å€¤ã‚’è¨ˆç®—ã—ã¦ãƒ•ãƒªãƒƒã‚«ãƒ¼é™¤å»
    return np.median(frames, axis=0).astype(np.uint8)
```

**ä»®æƒ³ã‚«ãƒ¡ãƒ©é™¤å¤–:**
```python
EXCLUDED_CAMERA_KEYWORDS = ["obs", "virtual", "screen capture"]
# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿æ™‚ã«OBSä»®æƒ³ã‚«ãƒ¡ãƒ©ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†
kill_virtual_camera_processes()
```

---

### ğŸŸ¡ ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡º

#### [yolo_processor.py](file:///Users/asanolab/Sotsusei1107/Assets/StreamingAssets/yolo_processor.py)
**å½¹å‰²:** YOLOv11ã«ã‚ˆã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºã¨ã‚¯ãƒ­ãƒƒãƒ—å‡¦ç†

**YOLOProcessorã‚¯ãƒ©ã‚¹:**

| ãƒ¡ã‚½ãƒƒãƒ‰ | å‹•ä½œ |
|:---|:---|
| `__init__()` | ãƒ¢ãƒ‡ãƒ«åã€ä¿¡é ¼åº¦é–¾å€¤ã€ãƒãƒ¼ã‚¸ãƒ³æ¯”ç‡ã‚’è¨­å®š |
| `initialize()` | YOLOãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰ |
| `detect_and_crop(image)` | æ¤œå‡ºæ•°ã«å¿œã˜ãŸã‚¯ãƒ­ãƒƒãƒ—å‡¦ç† |
| `_crop_with_margin()` | 10%ãƒãƒ¼ã‚¸ãƒ³ä»˜ãã§ã‚¯ãƒ­ãƒƒãƒ— |

**æ¤œå‡ºæ•°ã«å¿œã˜ãŸå‡¦ç†:**

| æ¤œå‡ºæ•° | å‡¦ç† |
|:---|:---|
| 0 | å…ƒç”»åƒã‚’ãã®ã¾ã¾ä½¿ç”¨ |
| 1 | å˜ä¸€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚¯ãƒ­ãƒƒãƒ—ï¼ˆ10%ãƒãƒ¼ã‚¸ãƒ³ï¼‰ |
| 2+ | å…¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å«ã‚€çµ±åˆãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã§ã‚¯ãƒ­ãƒƒãƒ— |

```python
def detect_and_crop(self, image):
    results = self.model(image, conf=self.confidence_threshold)
    detections = [...]  # æ¤œå‡ºçµæœã‚’åé›†
    
    if len(detections) == 0:
        return image, {"crop_type": "none"}
    elif len(detections) == 1:
        return self._crop_with_margin(...), {"crop_type": "single"}
    else:
        # å…¨æ¤œå‡ºã‚’å«ã‚€çµ±åˆãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹
        min_x1 = min(d["x1"] for d in detections)
        # ...
        return self._crop_with_margin(...), {"crop_type": "multi"}
```

---

### ğŸ”´ AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

#### [ollama_client.py](file:///Users/asanolab/Sotsusei1107/Assets/StreamingAssets/ollama_client.py)
**å½¹å‰²:** ãƒ­ãƒ¼ã‚«ãƒ«Ollamaï¼ˆqwen2.5vl:7bï¼‰ã¸ã®ç”»åƒåˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆ

**åˆ†æãƒ•ãƒ­ãƒ¼:**
1. ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
2. `prompts.ANALYSIS_PROMPT` ã¨å…±ã«Ollamaã¸é€ä¿¡
3. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONæŠ½å‡º
4. ã‚­ãƒ¼æ­£è¦åŒ–ï¼ˆã‚¿ã‚¤ãƒä¿®æ­£ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®šï¼‰

**å‡ºåŠ›å½¢å¼:**
```json
{
  "is_machine": true/false,
  "shape": "Round/Sharp/Square/Other",
  "state": "Old/New/Dirty/Broken/Normal",
  "item_name": "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå"
}
```

**ã‚­ãƒ¼æ­£è¦åŒ–ãƒ­ã‚¸ãƒƒã‚¯:**
```python
def _normalize_keys(self, data):
    # ã‚¿ã‚¤ãƒä¿®æ­£
    if "is_is_machine" in data:
        data["is_machine"] = data.pop("is_is_machine")
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
    data.setdefault("is_machine", False)
    data.setdefault("shape", "Other")
    data.setdefault("state", "Normal")
    data.setdefault("item_name", "Unknown Object")
    return data
```

---

#### [deepseek_client.py](file:///Users/asanolab/Sotsusei1107/Assets/StreamingAssets/deepseek_client.py)
**å½¹å‰²:** DeepSeek APIã¸ã®ã‚»ãƒªãƒ•ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ

**ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ§‹é€ :**
```python
def generate_dialogue(self, item_name, context_str, topic, obsession_instruction=None):
    full_prompt = (
        f"Role: Personify the object '{item_name}'.\n"
        f"{context_str}\n"
        f"Topic: {topic}\n\n"
        f"{prompts.CORE_LOGIC}\n"
        f"{obsession_instruction if obsession_instruction else ''}\n"
        f"{prompts.PERSONA_LOGIC}\n"
        f"{prompts.GEMINI_TASK}\n"
    )
    
    response = self.client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "You are the voice of an object..."},
            {"role": "user", "content": full_prompt}
        ],
        temperature=1.0  # é«˜ã„å‰µé€ æ€§
    )
    return response.choices[0].message.content.strip()
```

**å‡ºåŠ›å½¢å¼:**
```
ã‚»ãƒªãƒ•å†…å®¹ï¼ˆ60æ–‡å­—ä»¥å†…ï¼‰ by æ»ã£ãŸåå‰
```

---

#### [voice_client.py](file:///Users/asanolab/Sotsusei1107/Assets/StreamingAssets/voice_client.py)
**å½¹å‰²:** COEIROINKï¼ˆãƒ­ãƒ¼ã‚«ãƒ«TTSï¼‰ã¸ã®éŸ³å£°åˆæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ:** `http://localhost:50032/v1/synthesis`

---

### ğŸŸ£ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š

#### [prompts.py](file:///Users/asanolab/Sotsusei1107/Assets/StreamingAssets/prompts.py)
**å½¹å‰²:** åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ãƒˆãƒ”ãƒƒã‚¯ãƒªã‚¹ãƒˆã€ãƒšãƒ«ã‚½ãƒŠãƒ­ã‚¸ãƒƒã‚¯ã‚’å®šç¾©

**ä¸»è¦å®šæ•°:**

| å®šæ•°å | ç”¨é€” |
|:---|:---|
| `ANALYSIS_PROMPT` | Ollamaåˆ†æç”¨CoTãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ |
| `TOPIC_LIST` | ãƒ©ãƒ³ãƒ€ãƒ é¸æŠã•ã‚Œã‚‹ãƒˆãƒ”ãƒƒã‚¯ï¼ˆ25ç¨®é¡ï¼‰ |
| `CORE_LOGIC` | è¨˜æ†¶ã¨çŠ¶æ…‹æå†™é‡è¦–ã®æŒ‡ç¤º |
| `PERSONA_LOGIC` | ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«ã®å£èª¿å·® |
| `GEMINI_TASK` | 60æ–‡å­—åˆ¶é™ãƒ»æ»ã£ãŸåå‰ã®å‡ºåŠ›å½¢å¼ |

**ANALYSIS_PROMPT (Chain-of-Thoughtå½¢å¼):**
```
**Step 1: OBSERVATION**
List the visual features you observe...

**Step 2: REASONING**
Based on your observations, explain...

**Step 3: FINAL ANSWER**
Output your conclusion in strict JSON format...
```

**TOPIC_LIST (ä¸€éƒ¨æŠœç²‹):**
- "A recent time you were used"
- "Being used after a long time of not being touched"
- "The quiet time when you weren't being used"
- "The feeling of the owner's hands"
- "Getting a scratch or stain"

---

#### [item_obsessions.py](file:///Users/asanolab/Sotsusei1107/Assets/StreamingAssets/item_obsessions.py)
**å½¹å‰²:** ã‚¢ã‚¤ãƒ†ãƒ åˆ¥ã€ŒåŸ·ç€ã€æŒ‡ç¤ºã®å®šç¾©

ç‰¹å®šã®ã‚¢ã‚¤ãƒ†ãƒ åã«å¯¾ã—ã¦ã€ã‚ˆã‚Šå…·ä½“çš„ãªæ€§æ ¼ä»˜ã‘ã‚„è©±ã—æ–¹ã®æŒ‡ç¤ºã‚’æä¾›ã€‚

---

## 4. ç”»åƒå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```mermaid
graph LR
    subgraph Capture["1. ã‚«ãƒ¡ãƒ©æ’®å½±"]
        A[ã‚«ãƒ¡ãƒ©] --> B[5ãƒ•ãƒ¬ãƒ¼ãƒ æ¨ã¦\néœ²å‡ºå®‰å®š]
        B --> C[5ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—]
        C --> D[ä¸­å¤®å€¤åˆæˆ\nãƒ•ãƒªãƒƒã‚«ãƒ¼é™¤å»]
    end

    subgraph YOLO["2. YOLOæ¤œå‡º"]
        D --> E{æ¤œå‡ºæ•°}
        E -->|0| F[å…ƒç”»åƒä½¿ç”¨]
        E -->|1| G[å˜ä¸€ã‚¯ãƒ­ãƒƒãƒ—\n10%ãƒãƒ¼ã‚¸ãƒ³]
        E -->|2+| H[çµ±åˆBB\nã‚¯ãƒ­ãƒƒãƒ—]
    end

    subgraph Preprocess["3. å‰å‡¦ç†"]
        F --> I[æ˜ã‚‹ã•èª¿æ•´]
        G --> I
        H --> I
        I --> J[CLAHE\nã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´]
        J --> K[rembg\nèƒŒæ™¯é™¤å»]
    end

    subgraph Analysis["4. åˆ†æ"]
        K --> L[Ollama\nç”»åƒåˆ†æ]
        L --> M[JSONå‡ºåŠ›]
    end
```

**æ˜ã‚‹ã•èª¿æ•´ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆapply_intelligent_brightnessï¼‰:**
```python
def apply_intelligent_brightness(image):
    # 1. ã‚¬ãƒ³ãƒè£œæ­£ (gamma=1.5)
    gamma = 1.5
    inv_gamma = 1.0 / gamma
    table = np.array([((i / 255.0) ** inv_gamma) * 255 
                      for i in range(256)]).astype("uint8")
    result = cv2.LUT(image, table)
    
    # 2. å¹³å‡è¼åº¦ãŒä½ã„å ´åˆã¯åº•ä¸Šã’
    mean_brightness = np.mean(cv2.cvtColor(result, cv2.COLOR_BGR2GRAY))
    if mean_brightness < 100:
        result = cv2.add(result, np.array([30.0]))
    
    return result
```

---

## 5. ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 

```
StreamingAssets/
â”œâ”€â”€ main_vision_voice.py      # ãƒ¡ã‚¤ãƒ³ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
â”œâ”€â”€ camera_capture.py         # ã‚«ãƒ¡ãƒ©åˆ¶å¾¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”œâ”€â”€ yolo_processor.py         # YOLOæ¤œå‡ºãƒ»ã‚¯ãƒ­ãƒƒãƒ—
â”œâ”€â”€ ollama_client.py          # Ollamaç”»åƒåˆ†æã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
â”œâ”€â”€ deepseek_client.py        # DeepSeekã‚»ãƒªãƒ•ç”Ÿæˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
â”œâ”€â”€ voice_client.py           # COEIROINKéŸ³å£°åˆæˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
â”œâ”€â”€ prompts.py                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾©
â”œâ”€â”€ item_obsessions.py        # ã‚¢ã‚¤ãƒ†ãƒ åˆ¥åŸ·ç€æŒ‡ç¤º
â”œâ”€â”€ config.json               # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ .env                      # ç’°å¢ƒå¤‰æ•°ï¼ˆAPI Keyï¼‰
â”œâ”€â”€ yolo11n.pt                # YOLOãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ capture/                  # å‡¦ç†æ¸ˆã¿ç”»åƒä¿å­˜
â”‚   â””â”€â”€ raw/                  # å‡¦ç†å‰ã®å…ƒç”»åƒ
â”œâ”€â”€ voice/                    # ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ Message.txt               # ç”Ÿæˆã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ­ã‚°
â””â”€â”€ MessagePairs.json         # ç”»åƒã¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒšã‚¢æƒ…å ±
```

---

## 6. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

### config.json
```json
{
  "VOICE_VARIANTS": {
    "Neutral": {"uuid": "...", "style": "default"},
    "Observer": {"uuid": "...", "style": "calm"},
    // ...
  },
  "PERSONALITY_PROMPTS": {
    // ãƒšãƒ«ã‚½ãƒŠåˆ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¿½åŠ æŒ‡ç¤º
  },
  "PSYCHOLOGICAL_TRIGGERS": [
    // å¿ƒç†çš„ãƒˆãƒªã‚¬ãƒ¼ãƒªã‚¹ãƒˆ
  ]
}
```

### .env
```
DEEPSEEK_API_KEY=sk-xxxxx
```

---

## 7. ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

```bash
pip install ultralytics opencv-python rembg onnxruntime ollama watchdog openai python-dotenv
```

| ãƒ©ã‚¤ãƒ–ãƒ©ãƒª | ç”¨é€” |
|:---|:---|
| `ultralytics` | YOLOv11 |
| `opencv-python` | ç”»åƒå‡¦ç† |
| `rembg` | èƒŒæ™¯é™¤å» |
| `onnxruntime` | rembgã®æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ |
| `ollama` | ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ |
| `watchdog` | ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦– |
| `openai` | DeepSeek APIï¼ˆOpenAIäº’æ›ï¼‰ |
| `python-dotenv` | ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ |

---

## 8. é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [CSharpScriptLogic.md](./CSharpScriptLogic.md) - Unity C# ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ§‹é€ 
- [WorkflowDiagram.md](./WorkflowDiagram.md) - å…¨ä½“ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å›³
