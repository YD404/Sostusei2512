# プロジェクト概要書: Mono-Logue

本ドキュメントでは、本プロジェクト「Mono-Logue」（卒業制作）の概要、体験フロー、およびシステム構成の全体像についてまとめます。

## 1. プロジェクト・コンセプト
本システムは、**「モノ（物体）」の視点や思考をAIによって可視化・言語化するインタラクティブ・インスタレーション**です。
体験者が提示した物体をカメラで認識し、その物体が「何を考え、何を語りかけようとしているか」をリアルタイムに生成して提示します。

## 2. 体験フロー (User Experience Flow)

体験は以下の5つのフェーズで進行します。

### ① 待機 (Waiting)
- **画面**: マトリックス風に、過去の「モノの言葉」が画面上を流れ続けます。
- **状態**: システムは体験者の入力を待っています。

### ② 撮影 (Capture)
- **アクション**: 体験者が `Space` キーを押します。
- **処理**: Webカメラが目の前の物体を撮影し、画像を保存します。
- **演出**: 撮影音が鳴り、解析フェーズへ移行します。

### ③ 解析・生成 (Scanning)
- **画面**: 「Scanning...」「Analyzing image...」といった解析中画面が表示されます。
- **裏側の処理**:
  1. **画像認識 (Vision AI)**: 画像に何が写っているかを解析します。
  2. **台詞生成 (LLM)**: 解析結果に基づき、「そのモノなら言いそうなこと」を生成します。
  3. **音声合成 (TTS)**: 生成されたセリフを音声データに変換します。

### ④ 独白 (Monologue / Message)
- **画面**: 生成されたセリフがタイプライターアニメーションで表示されます。
- **音声**: 「モノ」の声でセリフが読み上げられます。
- **演出**:
  - **ルーン文字**: 言葉が空間に浮かび上がるようなエフェクト（Rune Effect）が発生します。
  - **サブディスプレイ**: 片隅でログとして記録されます。

### ⑤ 終了・余韻 (End)
- **画面**: セリフ表示が終了した後、短い余韻（End演出）を挟みます。
- **循環**: 再び「待機状態」に戻り、次の体験者を待ちます。

---

## 3. システム構成概要

本システムは、**Unity (フロントエンド)** と **Python (バックエンド)** の2つのプロセスが連携して動作します。

### Unity (演出・制御)
体験者が目にする全てのビジュアル、UI、入力を担当します。
- **役割**: 画面表示、キー入力検知、カメラ撮影、エフェクト再生
- **動作**: Pythonからの指示（メッセージ受信、状態遷移）に従って画面を切り替えます。

### Python (知能・生成)
裏で動作し、Unityから渡された画像を元にAI処理を行います。
- **役割**: 画像認識(Ollama/Vision)、テキスト生成(Gemini/Claude等)、音声合成(COEIROINK/TTS)
- **動作**: 処理結果を標準出力（ログ）としてUnityに送信します。

## 4. ディレクトリ構成の意図

- **Assets/Scripts/**: Unity側の全動作スクリプト
- **Assets/StreamingAssets/**: 
  - `capture/`: 撮影された画像の一時保存場所（Unity→Pythonの受け渡し場所）
  - `voice/`: 合成された音声ファイルの保存場所（Python→Unityの受け渡し場所）
  - `Message.txt`: 生成されたセリフの履歴ログ
  - `main_vision_voice.py`: AI処理を行うPythonスクリプト本体

---
*このドキュメントは、コード解析およびシステム挙動に基づいて作成された概要書です。*
